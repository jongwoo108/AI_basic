{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 딥 러닝을 이용한 자연어 처리 입문"
      ],
      "metadata": {
        "id": "lT9z0RHLMwPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[구글 코랩에서 실행하기](https://colab.research.google.com/drive/1WQSkfs-BU9ovWMXgFoiJH5Ap7IHV2zth?usp=sharing)"
      ],
      "metadata": {
        "id": "bVnsnMKJNU7J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://wikidocs.net/111472"
      ],
      "metadata": {
        "id": "PTKc0XWFNcrH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "06-04 자동 미분과 선형 회귀 실습"
      ],
      "metadata": {
        "id": "ulOyxj8wNeTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "선형 회귀를 텐서플로우와 케라스를 통해 구현해봅시다."
      ],
      "metadata": {
        "id": "7JWwx6b3Nf2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 자동 미분"
      ],
      "metadata": {
        "id": "pQZvAH2GNhcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "w49fDlpMNijk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tape_gradient()는 자동 미분 기능을 수행"
      ],
      "metadata": {
        "id": "BqHOpq7YNk5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "임의로 2w^2 + 5라는 식을세우고 w에 대해 미분"
      ],
      "metadata": {
        "id": "IIJbT_O6Nmq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = tf.Variable(2.)"
      ],
      "metadata": {
        "id": "S_WxhJYsNt6Q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f(w):\n",
        "    y = w**2\n",
        "    z = 2*y + 5\n",
        "    return z"
      ],
      "metadata": {
        "id": "o1FbrDmTNwBH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "gradients를 출력하면 w\n",
        "에 대해 미분한 값이 저장된 것을 확인"
      ],
      "metadata": {
        "id": "9R4cNZbGN4NF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.GradientTape() as tape:\n",
        "    z = f(w)\n",
        "\n",
        "gradients = tape.gradient(z, [w])\n",
        "print(gradients)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKUN5_91N6Rg",
        "outputId": "37164609-e4b3-4d5c-8154-0d00dc9082e7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<tf.Tensor: shape=(), dtype=float32, numpy=8.0>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 자동 미분 기능을 사용하여 선형 회귀를 구현"
      ],
      "metadata": {
        "id": "xTChcPd8OEqA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 자동 미분을 이용한 선형 회귀 구현"
      ],
      "metadata": {
        "id": "7VypiqNOOKL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "우선 가중치 변수 w와 b를 선언\\\n",
        "학습될 값이므로 임의의 값인 4와 1로 초기화"
      ],
      "metadata": {
        "id": "6YvdMtnrOMa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습될 가중치 변수를 선언\n",
        "w = tf.Variable(4.0)\n",
        "b = tf.Variable(1.0)"
      ],
      "metadata": {
        "id": "P0YizwihOODQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "가설을 함수로서 정의"
      ],
      "metadata": {
        "id": "3DEhtiqfOfQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def hypothesis(x):\n",
        "    return w*x + b"
      ],
      "metadata": {
        "id": "phA65KyfOg30"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "현재의 가설에서 w와 b는 각각 4와 1이므로 임이의 입력값을 넣었을 때의 결과는 다음과 같다."
      ],
      "metadata": {
        "id": "ASvmNPHXOmEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = [3.5, 5, 5.5, 6]\n",
        "print(hypothesis(x_test).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MV0I2YGOqwx",
        "outputId": "d0365efc-ec57-4eeb-9168-d0a5a3110f53"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15. 21. 23. 25.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "다음과 같이 평균 제곱 오차를 손실 함수로서 정의"
      ],
      "metadata": {
        "id": "Om0TKvJpO41j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def mse_loss(y_pred, y):\n",
        "    #두 개의 차이값을 제곱을 해서 평균을 취한다.\n",
        "    return tf.reduce_mean(tf.square(y_pred - y))"
      ],
      "metadata": {
        "id": "qtzRIHrNQBzD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기서 사용할 데이터는 x와 y가 약 10배의 차이를 가지는 데이터"
      ],
      "metadata": {
        "id": "FKrB4nyXQiYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = [1, 2, 3, 4, 5, 6, 7, 8, 9] #공부하는 시간\n",
        "y = [11, 22, 33, 44, 53, 66, 77, 87, 95] # 각 공부하는 시간에 맵핑되는 성적"
      ],
      "metadata": {
        "id": "7JWQFJ2aQlU-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "옵티마이저는 경사 하강법을 사용하되, 학습률(learning rate)는 0.01을 사용"
      ],
      "metadata": {
        "id": "tLSWseeqQund"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.optimizers.SGD(0.01)"
      ],
      "metadata": {
        "id": "FbQp9bCJQzQo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "약 300번에 걸쳐서 경사 하강법을 수행"
      ],
      "metadata": {
        "id": "f10ncngvQ1xP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(301):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # 현재 파라미터에 기반한 입력 x에 대한 예측값을 y_pred\n",
        "        y_pred = hypothesis(x)\n",
        "\n",
        "        # 평균 제곱 오차를 계산\n",
        "        cost = mse_loss(y_pred, y)\n",
        "\n",
        "    # 손실 함수에 대한 파라미터의 미분값 계산\n",
        "    gradients = tape.gradient(cost, [w, b])\n",
        "\n",
        "    # 파라미터 업데이트\n",
        "    optimizer.apply_gradients(zip(gradients, [w, b]))\n",
        "\n",
        "    if i % 10 == 0:\n",
        "        print(\"epoch : {:3} | w의 값 : {:5.4f} | b의 값 : {:5.4} | cost : {:5.6f}\".format(i, w.numpy(), b.numpy(), cost))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2PTo9O_Q3on",
        "outputId": "c80ed4bf-c746-4a6d-bb2a-7bd0c06e9f64"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch :   0 | w의 값 : 8.2133 | b의 값 : 1.664 | cost : 1402.555542\n",
            "epoch :  10 | w의 값 : 10.4971 | b의 값 : 1.977 | cost : 1.351182\n",
            "epoch :  20 | w의 값 : 10.5047 | b의 값 :  1.93 | cost : 1.328165\n",
            "epoch :  30 | w의 값 : 10.5119 | b의 값 : 1.884 | cost : 1.306967\n",
            "epoch :  40 | w의 값 : 10.5188 | b의 값 : 1.841 | cost : 1.287436\n",
            "epoch :  50 | w의 값 : 10.5254 | b의 값 : 1.799 | cost : 1.269459\n",
            "epoch :  60 | w의 값 : 10.5318 | b의 값 : 1.759 | cost : 1.252898\n",
            "epoch :  70 | w의 값 : 10.5379 | b의 값 : 1.721 | cost : 1.237644\n",
            "epoch :  80 | w의 값 : 10.5438 | b의 값 : 1.684 | cost : 1.223598\n",
            "epoch :  90 | w의 값 : 10.5494 | b의 값 : 1.648 | cost : 1.210658\n",
            "epoch : 100 | w의 값 : 10.5548 | b의 값 : 1.614 | cost : 1.198740\n",
            "epoch : 110 | w의 값 : 10.5600 | b의 값 : 1.582 | cost : 1.187767\n",
            "epoch : 120 | w의 값 : 10.5650 | b의 값 :  1.55 | cost : 1.177665\n",
            "epoch : 130 | w의 값 : 10.5697 | b의 값 :  1.52 | cost : 1.168354\n",
            "epoch : 140 | w의 값 : 10.5743 | b의 값 : 1.492 | cost : 1.159782\n",
            "epoch : 150 | w의 값 : 10.5787 | b의 값 : 1.464 | cost : 1.151890\n",
            "epoch : 160 | w의 값 : 10.5829 | b의 값 : 1.437 | cost : 1.144619\n",
            "epoch : 170 | w의 값 : 10.5870 | b의 값 : 1.412 | cost : 1.137924\n",
            "epoch : 180 | w의 값 : 10.5909 | b의 값 : 1.387 | cost : 1.131752\n",
            "epoch : 190 | w의 값 : 10.5946 | b의 값 : 1.364 | cost : 1.126073\n",
            "epoch : 200 | w의 값 : 10.5982 | b의 값 : 1.341 | cost : 1.120843\n",
            "epoch : 210 | w의 값 : 10.6016 | b의 값 :  1.32 | cost : 1.116026\n",
            "epoch : 220 | w의 값 : 10.6049 | b의 값 : 1.299 | cost : 1.111589\n",
            "epoch : 230 | w의 값 : 10.6081 | b의 값 : 1.279 | cost : 1.107504\n",
            "epoch : 240 | w의 값 : 10.6111 | b의 값 :  1.26 | cost : 1.103736\n",
            "epoch : 250 | w의 값 : 10.6140 | b의 값 : 1.242 | cost : 1.100273\n",
            "epoch : 260 | w의 값 : 10.6168 | b의 값 : 1.224 | cost : 1.097082\n",
            "epoch : 270 | w의 값 : 10.6195 | b의 값 : 1.207 | cost : 1.094143\n",
            "epoch : 280 | w의 값 : 10.6221 | b의 값 : 1.191 | cost : 1.091434\n",
            "epoch : 290 | w의 값 : 10.6245 | b의 값 : 1.176 | cost : 1.088940\n",
            "epoch : 300 | w의 값 : 10.6269 | b의 값 : 1.161 | cost : 1.086645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "w와 b값이 계속 업데이트 됨에 따라서 cost가 지속적으로 줄어드는 것을 확인할 수 있습니다. 학습된 w와 b의 값에 대해서 임의 입력을 넣었을 경우의 예측값을 확인해봅시다."
      ],
      "metadata": {
        "id": "4whL9WRYSyiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = [3.5, 5, 5.5, 6]\n",
        "print(hypothesis(x_test).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IH8P6_BwTC7F",
        "outputId": "35f9bee4-41b8-4891-bb1e-5096b9f30299"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[38.35479  54.295143 59.608593 64.92204 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델을 구현하는 방법은 한 가지가 아닙니다. 텐서플로우의 경우, 케라스라는 고수준의 API를 사용하면 모델을 이보다 좀 더 쉽게 구현할 수 있습니다. 이번에는 선형 회귀 모델을 케라스로 구현"
      ],
      "metadata": {
        "id": "LnaWZID8TMiI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. 케라스로 구현하는 선형 회귀"
      ],
      "metadata": {
        "id": "BWvRHjelTVlU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "간단하게 케라스를 이용해서 선형 회귀를 구현\\\n",
        "케라스로 모델을 만드는 기본적인 형식은 다음과 같습니다. Sequential로 model이라는 이름의 모델을 만들고, 그리고 add를 통해 입력과 출력 벡터의 차원과 같은 필요한 정보들을 추가"
      ],
      "metadata": {
        "id": "051zv_feTWwp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "첫번째 인자인 1은 출력의 차원을 정의합니다. 일반적으로 output_dim으로 표현되는 인자입니다. 두번째 인자인 input_dim은 입력의 차원을 정의하는데 이번 실습과 같이 1개의 실수\n",
        "를 가지고 하는 1개의 실수\n",
        "를 예측하는 단순 선형 회귀를 구현하는 경우에는 각각 1의 값을 가집니다.."
      ],
      "metadata": {
        "id": "HIJ1fKABTb5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # 예시 코드. 실행 불가.\n",
        "# model = Sequential()\n",
        "# model.add(keras.layers.Dense(1, input_dim=1))"
      ],
      "metadata": {
        "id": "ATl2V19YTgzt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "우선 공부한 시간을 x\n",
        ", 각 공부한 시간에 따른 성적을 y\n",
        "라고 해봅시다. activation은 어떤 함수를 사용할 것인지를 의미하는데 선형 회귀를 사용할 경우에는 linear라고 기재"
      ],
      "metadata": {
        "id": "haHxiak7Tk1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "옵티마이저로 기본 경사 하강법을 사용하고 싶다면, sgd라고 기재합니다. 학습률은 0.01로 정하였습니다. 손실 함수로는 평균 제곱 오차를 사 용합니다. 그리고 전체 데이터에 대한 훈련 횟수는 300으로 합니다."
      ],
      "metadata": {
        "id": "KE-QxmAITs8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import optimizers"
      ],
      "metadata": {
        "id": "VceJwwrQTvKo"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = [1, 2, 3, 4, 5, 6, 7, 8, 9] #공부하는 시간\n",
        "y = [11, 22, 33, 44, 53, 66, 77, 87, 95] #각 공부하는 시간에 맵핑되는 성적"
      ],
      "metadata": {
        "id": "2R_iVGWAT5Ro"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()"
      ],
      "metadata": {
        "id": "MZe4BA_nUEfa"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 출력 y의 차원은 1. 입력 x의 차원(input_dim)은 1\n",
        "# 선형 회귀이므로 activation은 'linear'\n",
        "model.add(Dense(1, input_dim = 1, activation = 'linear'))"
      ],
      "metadata": {
        "id": "M8ycoY_yUF70"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sgd는 경사 하강법을 의미. 학습률(learning rate, lr)은 0.01.\n",
        "sgd = optimizers.SGD(learning_rate=0.01)"
      ],
      "metadata": {
        "id": "K5JT9tKlUSCi"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#손실 함수(Loss function)은 평균제곱오차 mse를 사용.\n",
        "model.compile(optimizer=sgd, loss='mse', metrics = ['mse'])"
      ],
      "metadata": {
        "id": "60G6GE6TUZq9"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 주어진 x와 y 데이터에 대해서 오차를 최소화하는 작업을 300번 시도.\n",
        "model.fit(x, y, epochs = 300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFkefxA0Usjs",
        "outputId": "d576b919-1f79-4dfe-fe4e-0a1caa023891"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 4788.2490 - mse: 4788.2490\n",
            "Epoch 2/300\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 590.3092 - mse: 590.3092\n",
            "Epoch 3/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 73.7823 - mse: 73.7823\n",
            "Epoch 4/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 10.2257 - mse: 10.2257\n",
            "Epoch 5/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.4038 - mse: 2.4038\n",
            "Epoch 6/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.4396 - mse: 1.4396\n",
            "Epoch 7/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.3192 - mse: 1.3192\n",
            "Epoch 8/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.3026 - mse: 1.3026\n",
            "Epoch 9/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.2989 - mse: 1.2989\n",
            "Epoch 10/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2967 - mse: 1.2967\n",
            "Epoch 11/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.2947 - mse: 1.2947\n",
            "Epoch 12/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.2928 - mse: 1.2928\n",
            "Epoch 13/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.2909 - mse: 1.2909\n",
            "Epoch 14/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.2890 - mse: 1.2890\n",
            "Epoch 15/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.2871 - mse: 1.2871\n",
            "Epoch 16/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.2852 - mse: 1.2852\n",
            "Epoch 17/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.2834 - mse: 1.2834\n",
            "Epoch 18/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.2816 - mse: 1.2816\n",
            "Epoch 19/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.2797 - mse: 1.2797\n",
            "Epoch 20/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.2779 - mse: 1.2779\n",
            "Epoch 21/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2762 - mse: 1.2762\n",
            "Epoch 22/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2744 - mse: 1.2744\n",
            "Epoch 23/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.2726 - mse: 1.2726\n",
            "Epoch 24/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.2709 - mse: 1.2709\n",
            "Epoch 25/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2691 - mse: 1.2691\n",
            "Epoch 26/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2674 - mse: 1.2674\n",
            "Epoch 27/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.2657 - mse: 1.2657\n",
            "Epoch 28/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2640 - mse: 1.2640\n",
            "Epoch 29/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.2624 - mse: 1.2624\n",
            "Epoch 30/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.2607 - mse: 1.2607\n",
            "Epoch 31/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2591 - mse: 1.2591\n",
            "Epoch 32/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2574 - mse: 1.2574\n",
            "Epoch 33/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.2558 - mse: 1.2558\n",
            "Epoch 34/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2542 - mse: 1.2542\n",
            "Epoch 35/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2526 - mse: 1.2526\n",
            "Epoch 36/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.2510 - mse: 1.2510\n",
            "Epoch 37/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.2495 - mse: 1.2495\n",
            "Epoch 38/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2479 - mse: 1.2479\n",
            "Epoch 39/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.2464 - mse: 1.2464\n",
            "Epoch 40/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2448 - mse: 1.2448\n",
            "Epoch 41/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.2433 - mse: 1.2433\n",
            "Epoch 42/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.2418 - mse: 1.2418\n",
            "Epoch 43/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.2403 - mse: 1.2403\n",
            "Epoch 44/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.2388 - mse: 1.2388\n",
            "Epoch 45/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.2374 - mse: 1.2374\n",
            "Epoch 46/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2359 - mse: 1.2359\n",
            "Epoch 47/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2345 - mse: 1.2345\n",
            "Epoch 48/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2331 - mse: 1.2331\n",
            "Epoch 49/300\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2316 - mse: 1.2316\n",
            "Epoch 50/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.2302 - mse: 1.2302\n",
            "Epoch 51/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.2288 - mse: 1.2288\n",
            "Epoch 52/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.2274 - mse: 1.2274\n",
            "Epoch 53/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.2261 - mse: 1.2261\n",
            "Epoch 54/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.2247 - mse: 1.2247\n",
            "Epoch 55/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.2234 - mse: 1.2234\n",
            "Epoch 56/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.2220 - mse: 1.2220\n",
            "Epoch 57/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.2207 - mse: 1.2207\n",
            "Epoch 58/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2194 - mse: 1.2194\n",
            "Epoch 59/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2181 - mse: 1.2181\n",
            "Epoch 60/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.2168 - mse: 1.2168\n",
            "Epoch 61/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.2155 - mse: 1.2155\n",
            "Epoch 62/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.2142 - mse: 1.2142\n",
            "Epoch 63/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.2129 - mse: 1.2129\n",
            "Epoch 64/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.2117 - mse: 1.2117\n",
            "Epoch 65/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.2104 - mse: 1.2104\n",
            "Epoch 66/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2092 - mse: 1.2092\n",
            "Epoch 67/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2080 - mse: 1.2080\n",
            "Epoch 68/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.2068 - mse: 1.2068\n",
            "Epoch 69/300\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.2056 - mse: 1.2056\n",
            "Epoch 70/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.2044 - mse: 1.2044\n",
            "Epoch 71/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.2032 - mse: 1.2032\n",
            "Epoch 72/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.2020 - mse: 1.2020\n",
            "Epoch 73/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.2008 - mse: 1.2008\n",
            "Epoch 74/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1997 - mse: 1.1997\n",
            "Epoch 75/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1985 - mse: 1.1985\n",
            "Epoch 76/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1974 - mse: 1.1974\n",
            "Epoch 77/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1963 - mse: 1.1963\n",
            "Epoch 78/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1952 - mse: 1.1952\n",
            "Epoch 79/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1940 - mse: 1.1940\n",
            "Epoch 80/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.1929 - mse: 1.1929\n",
            "Epoch 81/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1919 - mse: 1.1919\n",
            "Epoch 82/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.1908 - mse: 1.1908\n",
            "Epoch 83/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1897 - mse: 1.1897\n",
            "Epoch 84/300\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.1886 - mse: 1.1886\n",
            "Epoch 85/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1876 - mse: 1.1876\n",
            "Epoch 86/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1865 - mse: 1.1865\n",
            "Epoch 87/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1855 - mse: 1.1855\n",
            "Epoch 88/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1845 - mse: 1.1845\n",
            "Epoch 89/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1834 - mse: 1.1834\n",
            "Epoch 90/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1824 - mse: 1.1824\n",
            "Epoch 91/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1814 - mse: 1.1814\n",
            "Epoch 92/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1804 - mse: 1.1804\n",
            "Epoch 93/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1794 - mse: 1.1794\n",
            "Epoch 94/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1785 - mse: 1.1785\n",
            "Epoch 95/300\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.1775 - mse: 1.1775\n",
            "Epoch 96/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1765 - mse: 1.1765\n",
            "Epoch 97/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1756 - mse: 1.1756\n",
            "Epoch 98/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1746 - mse: 1.1746\n",
            "Epoch 99/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1737 - mse: 1.1737\n",
            "Epoch 100/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1727 - mse: 1.1727\n",
            "Epoch 101/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1718 - mse: 1.1718\n",
            "Epoch 102/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1709 - mse: 1.1709\n",
            "Epoch 103/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1700 - mse: 1.1700\n",
            "Epoch 104/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1691 - mse: 1.1691\n",
            "Epoch 105/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1682 - mse: 1.1682\n",
            "Epoch 106/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1673 - mse: 1.1673\n",
            "Epoch 107/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1664 - mse: 1.1664\n",
            "Epoch 108/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1655 - mse: 1.1655\n",
            "Epoch 109/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1647 - mse: 1.1647\n",
            "Epoch 110/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1638 - mse: 1.1638\n",
            "Epoch 111/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1630 - mse: 1.1630\n",
            "Epoch 112/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1621 - mse: 1.1621\n",
            "Epoch 113/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1613 - mse: 1.1613\n",
            "Epoch 114/300\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.1605 - mse: 1.1605\n",
            "Epoch 115/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1596 - mse: 1.1596\n",
            "Epoch 116/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.1588 - mse: 1.1588\n",
            "Epoch 117/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1580 - mse: 1.1580\n",
            "Epoch 118/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1572 - mse: 1.1572\n",
            "Epoch 119/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1564 - mse: 1.1564\n",
            "Epoch 120/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1556 - mse: 1.1556\n",
            "Epoch 121/300\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.1548 - mse: 1.1548\n",
            "Epoch 122/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1541 - mse: 1.1541\n",
            "Epoch 123/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1533 - mse: 1.1533\n",
            "Epoch 124/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1525 - mse: 1.1525\n",
            "Epoch 125/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1518 - mse: 1.1518\n",
            "Epoch 126/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1510 - mse: 1.1510\n",
            "Epoch 127/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1503 - mse: 1.1503\n",
            "Epoch 128/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1495 - mse: 1.1495\n",
            "Epoch 129/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1488 - mse: 1.1488\n",
            "Epoch 130/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1480 - mse: 1.1480\n",
            "Epoch 131/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1473 - mse: 1.1473\n",
            "Epoch 132/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1466 - mse: 1.1466\n",
            "Epoch 133/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1459 - mse: 1.1459\n",
            "Epoch 134/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1452 - mse: 1.1452\n",
            "Epoch 135/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1445 - mse: 1.1445\n",
            "Epoch 136/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1438 - mse: 1.1438\n",
            "Epoch 137/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1431 - mse: 1.1431\n",
            "Epoch 138/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1424 - mse: 1.1424\n",
            "Epoch 139/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1418 - mse: 1.1418\n",
            "Epoch 140/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1411 - mse: 1.1411\n",
            "Epoch 141/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1404 - mse: 1.1404\n",
            "Epoch 142/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.1398 - mse: 1.1398\n",
            "Epoch 143/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1391 - mse: 1.1391\n",
            "Epoch 144/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1385 - mse: 1.1385\n",
            "Epoch 145/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1378 - mse: 1.1378\n",
            "Epoch 146/300\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1372 - mse: 1.1372\n",
            "Epoch 147/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1365 - mse: 1.1365\n",
            "Epoch 148/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1359 - mse: 1.1359\n",
            "Epoch 149/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1353 - mse: 1.1353\n",
            "Epoch 150/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1347 - mse: 1.1347\n",
            "Epoch 151/300\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1341 - mse: 1.1341\n",
            "Epoch 152/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1334 - mse: 1.1334\n",
            "Epoch 153/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1328 - mse: 1.1328\n",
            "Epoch 154/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1322 - mse: 1.1322\n",
            "Epoch 155/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1316 - mse: 1.1316\n",
            "Epoch 156/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1311 - mse: 1.1311\n",
            "Epoch 157/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1305 - mse: 1.1305\n",
            "Epoch 158/300\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.1299 - mse: 1.1299\n",
            "Epoch 159/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1293 - mse: 1.1293\n",
            "Epoch 160/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1288 - mse: 1.1288\n",
            "Epoch 161/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1282 - mse: 1.1282\n",
            "Epoch 162/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1276 - mse: 1.1276\n",
            "Epoch 163/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1271 - mse: 1.1271\n",
            "Epoch 164/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1265 - mse: 1.1265\n",
            "Epoch 165/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1260 - mse: 1.1260\n",
            "Epoch 166/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1254 - mse: 1.1254\n",
            "Epoch 167/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1249 - mse: 1.1249\n",
            "Epoch 168/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1244 - mse: 1.1244\n",
            "Epoch 169/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1238 - mse: 1.1238\n",
            "Epoch 170/300\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.1233 - mse: 1.1233\n",
            "Epoch 171/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1228 - mse: 1.1228\n",
            "Epoch 172/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1223 - mse: 1.1223\n",
            "Epoch 173/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1218 - mse: 1.1218\n",
            "Epoch 174/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1213 - mse: 1.1213\n",
            "Epoch 175/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1208 - mse: 1.1208\n",
            "Epoch 176/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1203 - mse: 1.1203\n",
            "Epoch 177/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1198 - mse: 1.1198\n",
            "Epoch 178/300\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.1193 - mse: 1.1193\n",
            "Epoch 179/300\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1188 - mse: 1.1188\n",
            "Epoch 180/300\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.1183 - mse: 1.1183\n",
            "Epoch 181/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1178 - mse: 1.1178\n",
            "Epoch 182/300\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.1173 - mse: 1.1173\n",
            "Epoch 183/300\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1169 - mse: 1.1169\n",
            "Epoch 184/300\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1164 - mse: 1.1164\n",
            "Epoch 185/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1159 - mse: 1.1159\n",
            "Epoch 186/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1155 - mse: 1.1155\n",
            "Epoch 187/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1150 - mse: 1.1150\n",
            "Epoch 188/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1146 - mse: 1.1146\n",
            "Epoch 189/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1141 - mse: 1.1141\n",
            "Epoch 190/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1137 - mse: 1.1137\n",
            "Epoch 191/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1132 - mse: 1.1132\n",
            "Epoch 192/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1128 - mse: 1.1128\n",
            "Epoch 193/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1124 - mse: 1.1124\n",
            "Epoch 194/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.1119 - mse: 1.1119\n",
            "Epoch 195/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1115 - mse: 1.1115\n",
            "Epoch 196/300\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1111 - mse: 1.1111\n",
            "Epoch 197/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1107 - mse: 1.1107\n",
            "Epoch 198/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1103 - mse: 1.1103\n",
            "Epoch 199/300\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.1098 - mse: 1.1098\n",
            "Epoch 200/300\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.1094 - mse: 1.1094\n",
            "Epoch 201/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1090 - mse: 1.1090\n",
            "Epoch 202/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1086 - mse: 1.1086\n",
            "Epoch 203/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1082 - mse: 1.1082\n",
            "Epoch 204/300\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1078 - mse: 1.1078\n",
            "Epoch 205/300\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1074 - mse: 1.1074\n",
            "Epoch 206/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1070 - mse: 1.1070\n",
            "Epoch 207/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1067 - mse: 1.1067\n",
            "Epoch 208/300\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.1063 - mse: 1.1063\n",
            "Epoch 209/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1059 - mse: 1.1059\n",
            "Epoch 210/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1055 - mse: 1.1055\n",
            "Epoch 211/300\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.1051 - mse: 1.1051\n",
            "Epoch 212/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1048 - mse: 1.1048\n",
            "Epoch 213/300\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.1044 - mse: 1.1044\n",
            "Epoch 214/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1040 - mse: 1.1040\n",
            "Epoch 215/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1037 - mse: 1.1037\n",
            "Epoch 216/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1033 - mse: 1.1033\n",
            "Epoch 217/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1030 - mse: 1.1030\n",
            "Epoch 218/300\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1026 - mse: 1.1026\n",
            "Epoch 219/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1023 - mse: 1.1023\n",
            "Epoch 220/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1019 - mse: 1.1019\n",
            "Epoch 221/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1016 - mse: 1.1016\n",
            "Epoch 222/300\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1012 - mse: 1.1012\n",
            "Epoch 223/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1009 - mse: 1.1009\n",
            "Epoch 224/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1005 - mse: 1.1005\n",
            "Epoch 225/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1002 - mse: 1.1002\n",
            "Epoch 226/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0999 - mse: 1.0999\n",
            "Epoch 227/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0996 - mse: 1.0996\n",
            "Epoch 228/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0992 - mse: 1.0992\n",
            "Epoch 229/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0989 - mse: 1.0989\n",
            "Epoch 230/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0986 - mse: 1.0986\n",
            "Epoch 231/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0983 - mse: 1.0983\n",
            "Epoch 232/300\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.0980 - mse: 1.0980\n",
            "Epoch 233/300\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.0976 - mse: 1.0976\n",
            "Epoch 234/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0973 - mse: 1.0973\n",
            "Epoch 235/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0970 - mse: 1.0970\n",
            "Epoch 236/300\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0967 - mse: 1.0967\n",
            "Epoch 237/300\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0964 - mse: 1.0964\n",
            "Epoch 238/300\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0961 - mse: 1.0961\n",
            "Epoch 239/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0958 - mse: 1.0958\n",
            "Epoch 240/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0955 - mse: 1.0955\n",
            "Epoch 241/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0952 - mse: 1.0952\n",
            "Epoch 242/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0949 - mse: 1.0949\n",
            "Epoch 243/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0947 - mse: 1.0947\n",
            "Epoch 244/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0944 - mse: 1.0944\n",
            "Epoch 245/300\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.0941 - mse: 1.0941\n",
            "Epoch 246/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0938 - mse: 1.0938\n",
            "Epoch 247/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0935 - mse: 1.0935\n",
            "Epoch 248/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0933 - mse: 1.0933\n",
            "Epoch 249/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0930 - mse: 1.0930\n",
            "Epoch 250/300\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0927 - mse: 1.0927\n",
            "Epoch 251/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0924 - mse: 1.0924\n",
            "Epoch 252/300\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0922 - mse: 1.0922\n",
            "Epoch 253/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0919 - mse: 1.0919\n",
            "Epoch 254/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0916 - mse: 1.0916\n",
            "Epoch 255/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0914 - mse: 1.0914\n",
            "Epoch 256/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0911 - mse: 1.0911\n",
            "Epoch 257/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0909 - mse: 1.0909\n",
            "Epoch 258/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0906 - mse: 1.0906\n",
            "Epoch 259/300\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0904 - mse: 1.0904\n",
            "Epoch 260/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0901 - mse: 1.0901\n",
            "Epoch 261/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0899 - mse: 1.0899\n",
            "Epoch 262/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0896 - mse: 1.0896\n",
            "Epoch 263/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0894 - mse: 1.0894\n",
            "Epoch 264/300\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0891 - mse: 1.0891\n",
            "Epoch 265/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0889 - mse: 1.0889\n",
            "Epoch 266/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0887 - mse: 1.0887\n",
            "Epoch 267/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0884 - mse: 1.0884\n",
            "Epoch 268/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0882 - mse: 1.0882\n",
            "Epoch 269/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0880 - mse: 1.0880\n",
            "Epoch 270/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0877 - mse: 1.0877\n",
            "Epoch 271/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0875 - mse: 1.0875\n",
            "Epoch 272/300\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0873 - mse: 1.0873\n",
            "Epoch 273/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0870 - mse: 1.0870\n",
            "Epoch 274/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0868 - mse: 1.0868\n",
            "Epoch 275/300\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.0866 - mse: 1.0866\n",
            "Epoch 276/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0864 - mse: 1.0864\n",
            "Epoch 277/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0862 - mse: 1.0862\n",
            "Epoch 278/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0860 - mse: 1.0860\n",
            "Epoch 279/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0857 - mse: 1.0857\n",
            "Epoch 280/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0855 - mse: 1.0855\n",
            "Epoch 281/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0853 - mse: 1.0853\n",
            "Epoch 282/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0851 - mse: 1.0851\n",
            "Epoch 283/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0849 - mse: 1.0849\n",
            "Epoch 284/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0847 - mse: 1.0847\n",
            "Epoch 285/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0845 - mse: 1.0845\n",
            "Epoch 286/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0843 - mse: 1.0843\n",
            "Epoch 287/300\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 1.0841 - mse: 1.0841\n",
            "Epoch 288/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0839 - mse: 1.0839\n",
            "Epoch 289/300\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0837 - mse: 1.0837\n",
            "Epoch 290/300\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0835 - mse: 1.0835\n",
            "Epoch 291/300\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0833 - mse: 1.0833\n",
            "Epoch 292/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0831 - mse: 1.0831\n",
            "Epoch 293/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0829 - mse: 1.0829\n",
            "Epoch 294/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0827 - mse: 1.0827\n",
            "Epoch 295/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0825 - mse: 1.0825\n",
            "Epoch 296/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0824 - mse: 1.0824\n",
            "Epoch 297/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0822 - mse: 1.0822\n",
            "Epoch 298/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0820 - mse: 1.0820\n",
            "Epoch 299/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0818 - mse: 1.0818\n",
            "Epoch 300/300\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0816 - mse: 1.0816\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x79c5f70017e0>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습이 끝났습니다. 최종적으로 선택된 오차를 최소화하는 직선을 그래프로 그려보겠습니다."
      ],
      "metadata": {
        "id": "h8XO_HORUwyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(x, model.predict(x), 'b', x, y, 'k.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "rOhfmYP_U500",
        "outputId": "4790f1bc-2cbc-4851-f283-ec2eac0ddc1d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 252ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x79c5f7003ca0>,\n",
              " <matplotlib.lines.Line2D at 0x79c5f7001750>]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위의 그래프에서 각 점은 우리가 실제 주었던 실제값에 해당되며, 직선은 실제값으로부터 오차를 최소화하는 w\n",
        "와 b\n",
        "의 값을 가지는 직선입니다. 이 직선을 통해 9시간 30분을 공부하였을 때의 시험 성적을 예측하게 해봅시다. model.predict()은 학습이 완료된 모델이 입력된 데이터에 대해서 어떤 값을 예측하는지를 보여줍니다."
      ],
      "metadata": {
        "id": "JkhX7EjrU9d8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.predict([9.5]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQnQcvU-VDJS",
        "outputId": "36580084-ff04-4fde-a4f4-0f44d09c402e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 39ms/step\n",
            "[[102.134026]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9시간 30분을 공부하면 약 102.1점을 얻는다고 예측"
      ],
      "metadata": {
        "id": "aoPpsM7VVI8x"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Eo8LBpRVUmL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}