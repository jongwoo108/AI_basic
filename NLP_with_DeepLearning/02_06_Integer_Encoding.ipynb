{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 딥 러닝을 이용한 자연어 처리 입문"
      ],
      "metadata": {
        "id": "nYa7DXpEvDzH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[구글 코랩에서 실행하기](https://colab.research.google.com/drive/1pAzTgqA7snt-DU3xK2pOT0JHcLudeeYK?usp=sharing)"
      ],
      "metadata": {
        "id": "aCYoVXcbvHBd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://wikidocs.net/31766"
      ],
      "metadata": {
        "id": "zII1NGSeHAh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "02-06 정수 인코딩(Integer Encoding)"
      ],
      "metadata": {
        "id": "sDTwcd36vNbG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 정수 인코딩(Integer Encoding)"
      ],
      "metadata": {
        "id": "M5ROVGCNvQMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) dictionary 사용하기"
      ],
      "metadata": {
        "id": "kLmRXtGgvUts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import gensim\n",
        "import os\n",
        "import nltk\n",
        "import sklearn\n",
        "\n",
        "\n",
        "print(tf.__version__)\n",
        "print(keras.__version__)\n",
        "print(gensim.__version__)\n",
        "print(sklearn.__version__)\n",
        "print(nltk.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StjjpV1Hv837",
        "outputId": "8d0d2762-1574-4375-d5b5-6c9a7ed38751"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n",
            "2.12.0\n",
            "4.3.1\n",
            "1.2.2\n",
            "3.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDCTiDkbwDsK",
        "outputId": "6ccdfe00-74ef-4680-d2af-773ab9ee9178"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install JPype1-0.6.3-cp36-cp36m-win_amd64.whl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1baqQ92wFrl",
        "outputId": "8258e7ae-5026-43d3-c803-6352caf53f40"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Requirement 'JPype1-0.6.3-cp36-cp36m-win_amd64.whl' looks like a filename, but the file does not exist\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: JPype1-0.6.3-cp36-cp36m-win_amd64.whl is not a supported wheel on this platform.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "eIY304UrwHmt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djlo_qNnxTfw",
        "outputId": "d0e64040-fee8-48dc-a343-0c1941adb535"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDnUQjIV3Yjv",
        "outputId": "f730fc54-cf9e-4a4e-ef8d-a941fc1ee4a1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret.\\\n",
        " Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret.\\\n",
        "  But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
      ],
      "metadata": {
        "id": "3b3ivgXywyn-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#문장 토큰화\n",
        "sentences = sent_tokenize(raw_text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9KixudOw2Xm",
        "outputId": "f469d3ec-a6d0-4a5f-9cc5-333fd51b3533"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "기존의 텍스트 데이터가 문장 단위로 토큰화\\\n",
        "이제 정제 작업과 정규화 작업을 병행\\\n",
        "단어 토큰화를 수행\\\n",
        "여기서는 단어들을 소문자화하여 단어의 개수를 통일시키고, 불용어와 단어 길이가 2이하인 경우에 대해서 단어를 일부 제외시켜주었습니다. 텍스트를 수치화하는 단계라는 것은 본격적으로 자연어 처리 작업에 들어간다는 의미이므로, 단어가 텍스트일 때만 할 수 있는 최대한의 전처리를 끝내놓아야 합니다.\n"
      ],
      "metadata": {
        "id": "XYdmQiMzxAY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {}\n",
        "preprocessed_sentences = []\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "t5JJo7vtxr-q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in sentences:\n",
        "  #단어 토큰화\n",
        "  tokenized_sentence = word_tokenize(sentence)\n",
        "  result = []\n",
        "\n",
        "  for word in tokenized_sentence:\n",
        "    word = word.lower() #모든 단어를 소문자화하여 단어의 개수를 줄인다.\n",
        "    if word not in stop_words: #단어 토큰화 된 결과에 대해서 불용어를 제거.\n",
        "      if len(word) > 2: #단어 길이가 2이하인 경우에 대하여 추가로 단어를 제거.\n",
        "        result.append(word)\n",
        "        if word not in vocab:\n",
        "          vocab[word] = 0\n",
        "        vocab[word] += 1\n",
        "  preprocessed_sentences.append(result)\n",
        "print(preprocessed_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giR1G9NFx80J",
        "outputId": "afd4c76e-55a8-4af9-d387-a1a0030e470d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('단어 집합 :', vocab)"
      ],
      "metadata": {
        "id": "N1ow7e_K4FgD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d929742-62e8-4731-a0f0-43f0ffe4b5f4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합 : {'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 'barber'라는 단어의 빈도수 출력\n",
        "print(vocab[\"barber\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wo4iwdMeDYb6",
        "outputId": "15cdc250-3329-4be4-c7fb-ef0cfcce73e3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse = True)\n",
        "print(vocab_sorted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVsnAqoeDgMu",
        "outputId": "9b57e512-1e8b-4b39-fdce-880ac2d1cfb8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index = {}\n",
        "i = 0\n",
        "for (word, frequency) in vocab_sorted:\n",
        "  if frequency > 1 :\n",
        "    i = i + 1\n",
        "    word_to_index[word] = i\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NB5rI6bbDrpa",
        "outputId": "09a87bc6-787f-47e2-e313-0f16febb5026"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1의 인덱스를 가진 단어가 가장 빈도수가 높은 단어가 됩니다. 그리고 이러한 작업을 수행하는 동시에 각 단어의 빈도수를 알 경우에만 할 수 있는 전처리인 빈도수가 적은 단어를 제외시키는 작업을 수행했습니다."
      ],
      "metadata": {
        "id": "3OHiFcYLEFzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 5"
      ],
      "metadata": {
        "id": "5S_BJNJ1EXOX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 인덱스가 5 초과인 단어 제거\n",
        "words_frequency = [word for word, index in word_to_index.items() if index >= vocab_size +1]"
      ],
      "metadata": {
        "id": "UnMgzd5EEZiD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 해당 단어에 대한 인덱스 정보를 삭제\n",
        "for w in words_frequency:\n",
        "  del word_to_index[w]\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGrAFVFMEbJF",
        "outputId": "0b254cd4-0ce0-444b-d8ea-bb52010ce496"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "word_to_index에는 빈도수가 높은 상위 5개의 단어만 저장"
      ],
      "metadata": {
        "id": "saDRSP7FEo2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index['OOV'] = len(word_to_index) + 1\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh3fdCdTEsGd",
        "outputId": "73ff3c1f-1347-4f6f-aec3-22e7bfebf7a6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'OOV': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "word_to_index를 사용하여 sentences의 모든 단어들을, 맵핑되는 정수로 인코딩"
      ],
      "metadata": {
        "id": "gNYV58XoEydg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_sentences = []\n",
        "for sentence in preprocessed_sentences:\n",
        "  encoded_sentence = []\n",
        "  for word in sentence:\n",
        "    try:\n",
        "      #단어 집합에 있는 단어라면 해당 단어의 정수를 리턴.\n",
        "      encoded_sentence.append(word_to_index[word])\n",
        "    except KeyError:\n",
        "      #만약 단어 집합에 없는 단어라면 'OOV'의 정수를 리턴.\n",
        "      encoded_sentence.append(word_to_index['OOV'])\n",
        "  encoded_sentences.append(encoded_sentence)\n",
        "print(encoded_sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8Ji92EbE3wM",
        "outputId": "1bc32b08-4aed-402c-8c1f-987883c7e9ff"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 5], [1, 6, 5], [1, 3, 5], [6, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [6, 6, 3, 2, 6, 1, 6], [1, 6, 3, 6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "파이썬의 dictionary 자료형으로 정수 인코딩을 진행해보았습니다. 그런데 이보다는 좀 더 쉽게 하기 위해서 Counter, FreqDist, enumerate를 사용하거나, 케라스 토크나이저를 사용하는 것을 권장"
      ],
      "metadata": {
        "id": "chCxx-PUFrNr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Counter 사용하기"
      ],
      "metadata": {
        "id": "HiYemSWfFz7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "print(preprocessed_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g41QhHbqF4rF",
        "outputId": "941a6fba-5c0e-4463-b959-48df92614490"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "현재 sentences는 단어 토큰화가 된 결과가 저장되어져 있습니다. 단어 집합(vocabulary)을 만들기 위해서 sentences에서 문장의 경계인 [, ]를 제거하고 단어들을 하나의 리스트로 만들겠습니다."
      ],
      "metadata": {
        "id": "qxC1mNFVF8Lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# words = np.hstack(preprocessed_sentences)으로도 수행 가능.\n",
        "all_words_list = sum(preprocessed_sentences, [])\n",
        "print(all_words_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqyURg3KGB61",
        "outputId": "38d83ca3-457e-45cb-d9b6-940a64c6cf48"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "파이썬의 Counter()의 입력으로 사용하면 중복을 제거하고 단어의 빈도수를 기록"
      ],
      "metadata": {
        "id": "gwqaBLTCGMjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 파이썬의 Counter 모듈을 이용하여 단어의 빈도수 카운트\n",
        "vocab = Counter(all_words_list)\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOpD85F-GWaJ",
        "outputId": "552af750-3f75-4863-d8d2-5197345fd3c3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(vocab['barber']) # 'barber'라는 단어의 빈도수 출력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ttJMqIoGdWM",
        "outputId": "b0636dd2-63a2-49cd-8d08-3d55072f9485"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "barber란 단어가 총 8번 등장. most_common()는 상위 빈도수를 가진 주어진 수의 단어만을 리턴, 이를 사용하여 등장 빈도수가 높은 단어들을 원하는 개수만큼 얻을 수 있다. 등장 빈도수 상위 5개의 단어만 단어 집합으로 저장."
      ],
      "metadata": {
        "id": "cDw2xxEDH8gC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 5\n",
        "vocab = vocab.most_common(vocab_size)\n",
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWlLsNhMIHcq",
        "outputId": "942cdd10-94b9-485c-e4fd-71a2848f5ec3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FoLMwQld4Zg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index = {}\n",
        "i = 0\n",
        "for (word, frequency) in vocab:\n",
        "  i = i + 1\n",
        "  word_to_index[word] = i\n",
        "\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdJO_6ZFILHr",
        "outputId": "03d38111-aac9-4f4a-f96b-1376edcbbd6d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK의 FreqDist 사용하기"
      ],
      "metadata": {
        "id": "XnDyMDx3IVSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import FreqDist\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "bIkID7-UIicA"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# np.hstack으로 문장 구분을 제거\n",
        "vocab = FreqDist(np.hstack(preprocessed_sentences))"
      ],
      "metadata": {
        "id": "dcq6GhkIImJc"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어를 키(key)로, 단어에 대한 빈도수가 값(value)으로 저장되어져 있다. vocab에 단어를 입력하면 빈도수를 리턴."
      ],
      "metadata": {
        "id": "A-mHl8CZIrcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab[\"barber\"]) #barber라는 빈도수 출력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jF7XS7QNIyKk",
        "outputId": "8dea1013-beba-498e-b066-be64c0b18b5b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "barber란 단어가 총 8번 등장. most_common()는 상위 빈도수를 가진 주어진 수의 단어만을 리턴.\\\n",
        "이를 사용하여 등장 빈도수가 높은 단어들을 원하는 개수만큼 얻을 수 있다.\\\n",
        "등장 빈도수 상위 5개의 단어만 단어 집합으로 저장"
      ],
      "metadata": {
        "id": "y-DoZK-cI2zv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 5\n",
        "vocab = vocab.most_common(vocab_size)\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2v2NTUaxJBfn",
        "outputId": "1d8485bb-4c32-4958-f067-2837eeb2a275"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "enumerate()사용"
      ],
      "metadata": {
        "id": "HAPhWd8JJF_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index = {word[0]:index + 1 for index, word in enumerate(vocab)}\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wF_qPSRnJLKB",
        "outputId": "06647683-02b7-4fa7-fc03-406bb027aca4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "enumerate 이해하기"
      ],
      "metadata": {
        "id": "L3v9Fgd3JS8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_input = ['a', 'b', 'c', 'd', 'e']\n",
        "for index, value in enumerate(test_input):\n",
        "  print(\"value : {}, index: {}\".format(value, index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70hkd7pBDYKF",
        "outputId": "1c69844d-3820-4458-d92a-c2f0c575e689"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "value : a, index: 0\n",
            "value : b, index: 1\n",
            "value : c, index: 2\n",
            "value : d, index: 3\n",
            "value : e, index: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "케라스(Keras)의 텍스트 전처리"
      ],
      "metadata": {
        "id": "YG68Q6VvFuQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "케라스(Keras)는 기본적인 전처리를 위한 도구들을 제공"
      ],
      "metadata": {
        "id": "BE4If7dSF7uE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "Vp6EycsTGAEo"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], \\\n",
        " ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], \\\n",
        "  ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]"
      ],
      "metadata": {
        "id": "pxKx6JLxGMv2"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()"
      ],
      "metadata": {
        "id": "vtt1ajOTGbFj"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성.\n",
        "tokenizer.fit_on_texts(preprocessed_sentences)"
      ],
      "metadata": {
        "id": "PQ8lHHu6GuDt"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fit_on_texts는 입력한 텍스트로부터 단어 빈도수가 높은 순으로 낮은 정수 인덱스를 부여하는데, 정확히 앞서 설명한 정수 인코딩 작업이 이루어진다고 보면됩니다. 각 단어에 인덱스가 어떻게 부여되었는지를 보려면, word_index를 사용합니다."
      ],
      "metadata": {
        "id": "8etajPAZG1R4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-6Er9fbG-P5",
        "outputId": "59efecfd-711f-463b-d17d-c232c8f01159"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "각 단어의 빈도수가 높은 순서대로 인덱스가 부여된 것을 확인 \\\n",
        "\n",
        "각 단어가 카운트를 수행하였을 때 몇 개였는지를 보고자 한다면 word_counts를 사용"
      ],
      "metadata": {
        "id": "RCHKPPIIHc2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.word_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nww7tn3oJPsX",
        "outputId": "0e61a093-4124-4803-ae6e-a140bc1301a4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "texts_to_sequences()는 입력으로 들어온 코퍼스에 대해서 각 단어를 이미 정해진 인덱스로 변환"
      ],
      "metadata": {
        "id": "c8xtREAHJU4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlCtbndEJfnn",
        "outputId": "ba64436a-03a4-4d8d-f36d-231f2eb80f2e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "빈도수가 가장 높은 단어 n개만을 사용하기 위해서 most_common()을 사용\\\n",
        "케라스 토크나이저에서는 tokenizer = Tokenizer(num_words=숫자)와 같은 방법으로 빈도수가 높은 상위 몇 개의 단어만 사용하겠다고 지정할 수 있습니다. \\\n",
        "여기서는 1번 단어부터 5번 단어까지만 사용하겠습니다. \\\n",
        "상위 5개 단어를 사용한다고 토크나이저를 재정의 해보겠습니다.\n"
      ],
      "metadata": {
        "id": "bje5sCOZJk2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 5\n",
        "tokenizer = Tokenizer(num_words = vocab_size + 1) # 상위 5개 단어만 사용\n",
        "tokenizer.fit_on_texts(preprocessed_sentences)"
      ],
      "metadata": {
        "id": "i9pbLwfqJ9Ee"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "num_words에서 +1을 더해서 값을 넣어주는 이유는 num_words는 숫자를 0부터 카운트\\\n",
        " 1 ~ 5번 단어까지 사용하고 싶다면 num_words에 숫자 5를 넣어주는 것이 아니라 5+1인 값을 넣어주어야 합니다.\n"
      ],
      "metadata": {
        "id": "pbaH8tHCKHHg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "실질적으로 숫자 0에 지정된 단어가 존재하지 않는데도 케라스 토크나이저가 숫자 0까지 단어 집합의 크기로 산정하는 이유는 \\\n",
        "자연어 처리에서 패딩(padding)이라는 작업 때문"
      ],
      "metadata": {
        "id": "FZG_WviQKPlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-mIDL1PKajK",
        "outputId": "83f33681-e267-487e-b38f-550c64bec9c1"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "상위 5개의 단어만 사용하겠다고 선언하였는데 여전히 13개의 단어가 모두 출력\\\n",
        "word_counts를 확인"
      ],
      "metadata": {
        "id": "70VPF4c7Kdwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.word_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIlKuvNJKlIm",
        "outputId": "112a55e7-358e-4361-9c6c-4fc85c7e46cb"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "word_counts에서도 마찬가지로 13개의 단어가 모두 출력\\\n",
        "실제 적용은 texts_to_sequences를 사용할 때 적용"
      ],
      "metadata": {
        "id": "IJPavOQ3KnOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iviMQwxOKuGO",
        "outputId": "60852d98-caa8-4e9b-81d9-5209a3b5a7ae"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "word_index와 word_counts에서도 지정된 num_words만큼의 단어만 남기고 싶다면 아래의 코드도 방법입니다."
      ],
      "metadata": {
        "id": "LFVVuEo2KyKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(preprocessed_sentences)"
      ],
      "metadata": {
        "id": "rgoHi-ZGMgeF"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 5\n",
        "words_frequency = [word for word, index in tokenizer.word_index.items() if index >= vocab_size + 1]"
      ],
      "metadata": {
        "id": "mz_3wP_DMl7W"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 인덱스가 5 초과인 단어 제거\n",
        "for word in words_frequency:\n",
        "  del tokenizer.word_index[word] #해당 단어에 대한 인덱스 정보를 삭제\n",
        "  del tokenizer.word_counts[word] # 해당 단어에 대한 카운트 정보를 삭제"
      ],
      "metadata": {
        "id": "JqRFy0n3M23v"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.word_index)\n",
        "print(tokenizer.word_counts)\n",
        "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uu91o4umNBlq",
        "outputId": "344584e4-eb49-4ef0-daa4-48a951c11617"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n",
            "OrderedDict([('barber', 8), ('person', 3), ('huge', 5), ('secret', 6), ('kept', 4)])\n",
            "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "케라스 토크나이저는 기본적으로 단어 집합에 없는 단어인 OOV에 대해서는 단어를 정수로 바꾸는 과정에서 아예 단어를 제거한다는 특징\\\n",
        "단어 집합에 없는 단어들은 OOV로 간주하여 보존하고 싶다면 Tokenizer의 인자 oov_token을 사용"
      ],
      "metadata": {
        "id": "eaFVhlLmNMDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 숫자 0과 OOV를 고려해서 단어 집합의 크기는 +2\n",
        "vocab_size = 5\n",
        "tokenizer = Tokenizer(num_words = vocab_size + 2, oov_token = 'OOV')\n",
        "tokenizer.fit_on_texts(preprocessed_sentences)"
      ],
      "metadata": {
        "id": "bnZ-bzCSOIL1"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "만약 oov_token을 사용하기로 했다면 케라스 토크나이저는 기본적으로 'OOV'의 인덱스를 1로 합니다."
      ],
      "metadata": {
        "id": "Domm7VIhOW05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('단어 OOV의 인덱스 : {}'.format(tokenizer.word_index['OOV']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_KWhdwBQse6",
        "outputId": "19c8e3d6-6fa6-4aa2-9c3c-c1d47e7cd19b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 OOV의 인덱스 : 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "코퍼스에 대한 정수 인코딩 진행"
      ],
      "metadata": {
        "id": "l-lGE8cnQzGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKu54WGYQ1Ru",
        "outputId": "b1b7d9f8-8f45-4ef7-8aca-f1260901895f"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ALTQeZ4TQ6t4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}